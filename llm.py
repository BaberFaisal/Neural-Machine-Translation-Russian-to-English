# -*- coding: utf-8 -*-
"""LLM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tSsit4aCeZ6mqtZRHCRsEVqj1-ySj3Lw

We shall compose encoder-decoder neural networks and apply them to the task of machine translation.

![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)
_(img: esciencegroup.files.wordpress.com)_


Encoder-decoder architectures are about converting anything to anything, including
 * Machine translation and spoken dialogue systems
 * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://openai.com/requests-for-research/#im2latex) (convolutional encoder, recurrent decoder)
 * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)
 * Grapheme2phoneme - convert words to transcripts

## Our task: machine translation

We gonna try our encoder-decoder models on russian to english machine translation problem. More specifically, we'll translate hotel and hostel descriptions. This task shows the scale of machine translation while not requiring we to train our model for weeks if you don't use GPU.

Before we get to the architecture, there's some preprocessing to be done. ~~Go tokenize~~ Alright, this time we've done preprocessing for you. As usual, the data will be tokenized with WordPunctTokenizer.

However, there's one more thing to do. Our data lines contain unique rare words. If we operate on a word level, we will have to deal with large vocabulary size. If instead we use character-level models, it would take lots of iterations to process a sequence. This time we're gonna pick something inbetween.

One popular approach is called [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) aka __BPE__. The algorithm starts with a character-level tokenization and then iteratively merges most frequent pairs for N iterations. This results in frequent words being merged into a single token and rare words split into syllables or even characters.
"""

!pip3 install torch>=1.3.0
!pip3 install subword-nmt &> log
!wget https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1 -O data.txt
!wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/week04_seq2seq/vocab.py -O vocab.py
# thanks to tilda and deephack teams for the data, Dmitry Emelyanenko for the code :)

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from copy import deepcopy
from IPython.display import clear_output
import matplotlib.pyplot as plt
from tqdm import trange

from nltk.tokenize import WordPunctTokenizer
from subword_nmt.learn_bpe import learn_bpe
from subword_nmt.apply_bpe import BPE
tokenizer = WordPunctTokenizer()
def tokenize(x):
    return ' '.join(tokenizer.tokenize(x.lower()))

# split and tokenize the data
with open('train.en', 'w') as f_src,  open('train.ru', 'w') as f_dst:
    for line in open('data.txt'):
        src_line, dst_line = line.strip().split('\t')
        f_src.write(tokenize(src_line) + '\n')
        f_dst.write(tokenize(dst_line) + '\n')

# build and apply bpe vocs
bpe = {}
for lang in ['en', 'ru']:
    learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)
    bpe[lang] = BPE(open('./bpe_rules.' + lang))

    with open('train.bpe.' + lang, 'w') as f_out:
        for line in open('train.' + lang):
            f_out.write(bpe[lang].process_line(line.strip()) + '\n')

"""### Building vocabularies

We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

data_inp = np.array(open('./train.bpe.ru').read().split('\n'))
data_out = np.array(open('./train.bpe.en').read().split('\n'))

from sklearn.model_selection import train_test_split
train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,
                                                          random_state=42)
for i in range(3):
    print('inp:', train_inp[i])
    print('out:', train_out[i], end='\n\n')

from vocab import Vocab
inp_voc = Vocab.from_lines(train_inp)
out_voc = Vocab.from_lines(train_out)

# Here's how you cast lines into ids and backwards.
batch_lines = sorted(train_inp, key=len)[5:10]
batch_ids = inp_voc.to_matrix(batch_lines)
batch_lines_restored = inp_voc.to_lines(batch_ids)

print("lines")
print(batch_lines)
print("\nwords to ids (0 = bos, 1 = eos):")
print(batch_ids)
print("\nback to words")
print(batch_lines_restored)

"""Draw source and translation length distributions to estimate the scope of the task."""

plt.figure(figsize=[8, 4])
plt.subplot(1, 2, 1)
plt.title("source length")
plt.hist(list(map(len, map(str.split, train_inp))), bins=20);

plt.subplot(1, 2, 2)
plt.title("translation length")
plt.hist(list(map(len, map(str.split, train_out))), bins=20);

"""### Encoder-decoder model

The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything. This model is implemented for you as a reference and a baseline for your homework assignment.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

class BasicModel(nn.Module):
    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):
        """
        A simple encoder-decoder seq2seq model
        """
        super().__init__()

        self.inp_voc, self.out_voc = inp_voc, out_voc
        self.hid_size = hid_size

        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)
        self.emb_out = nn.Embedding(len(out_voc), emb_size)
        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)

        self.dec_start = nn.Linear(hid_size, hid_size)
        self.dec0 = nn.GRUCell(emb_size, hid_size)
        self.logits = nn.Linear(hid_size, len(out_voc))

    def forward(self, inp, out):
        """ Apply model in training mode """
        initial_state = self.encode(inp)
        return self.decode(initial_state, out)


    def encode(self, inp, **flags):
        """
        Takes symbolic input sequence, computes initial state
        :param inp: matrix of input tokens [batch, time]
        :returns: initial decoder state tensors, one or many
        """
        inp_emb = self.emb_inp(inp)
        batch_size = inp.shape[0]

        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)
        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]

        # note: last_state is not _actually_ last because of padding, let's find the real last_state
        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)
        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]
        # ^-- shape: [batch_size, hid_size]

        dec_start = self.dec_start(last_state)
        return [dec_start]

    def decode_step(self, prev_state, prev_tokens, **flags):
        """
        Takes previous decoder state and tokens, returns new state and logits for next tokens
        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)
        :param prev_tokens: previous output tokens, an int vector of [batch_size]
        :return: a list of next decoder state tensors, a tensor of logits [batch, len(out_voc)]
        """
        prev_gru0_state = prev_state[0]
        if prev_tokens.device != prev_gru0_state.device:
            prev_tokens = prev_tokens.to(prev_gru0_state.device)
        prev_tokens = prev_tokens.long()
        prev_emb = self.emb_out(prev_tokens)
        new_gru0_state = self.dec0(prev_emb, prev_gru0_state)
        output_logits = self.logits(new_gru0_state)
        new_dec_state = [new_gru0_state]
        return new_dec_state, output_logits

    def decode(self, initial_state, out_tokens, **flags):
        """ Iterate over reference tokens (out_tokens) with decode_step """
        batch_size = out_tokens.shape[0]
        state = initial_state

        # initial logits: always predict BOS
        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),
                               num_classes=len(self.out_voc)).to(device=out_tokens.device)
        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)

        logits_sequence = [first_logits]
        for i in range(out_tokens.shape[1] - 1):
            state, logits = self.decode_step(state, out_tokens[:, i])
            logits_sequence.append(logits)
        return torch.stack(logits_sequence, dim=1)

    def decode_inference(self, initial_state, max_len=100, **flags):
        """ Generate translations from model (greedy version) """
        batch_size, device = len(initial_state[0]), initial_state[0].device
        state = initial_state
        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64,
                              device=device)]
        all_states = [initial_state]

        for i in range(max_len):
            state, logits = self.decode_step(state, outputs[-1])
            outputs.append(logits.argmax(dim=-1))
            all_states.append(state)

        return torch.stack(outputs, dim=1), all_states

    def translate_lines(self, inp_lines, **kwargs):
        inp = self.inp_voc.to_matrix(inp_lines).to(device)
        initial_state = self.encode(inp)
        out_ids, states = self.decode_inference(initial_state, **kwargs)
        return self.out_voc.to_lines(out_ids.cpu().numpy()), states

# debugging area
model = BasicModel(inp_voc, out_voc).to(device)

dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)
dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)

h0 = model.encode(dummy_inp_tokens)
h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))

assert isinstance(h1, list) and len(h1) == len(h0)
assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])
assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))

logits_seq = model.decode(h0, dummy_out_tokens)
assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))

logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)
assert logits_seq2.shape == logits_seq.shape

print("Source:")
print('\n'.join([line for line in train_inp[:3]]))
dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)

print("\nTranslations without training:")
print('\n'.join([line for line in dummy_translations]))

"""### Training loss

Our training objective is almost the same as it was for neural language models:
$$ L = {\frac1{|D|}} \sum_{X, Y \in D} \sum_{y_t \in Y} - \log p(y_t \mid y_1, \dots, y_{t-1}, X, \theta) $$

where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD.
"""

def compute_loss(model, inp, out, **flags):
    """
    Compute loss (float32 scalar) as in the formula above
    :param inp: input tokens matrix, int32[batch, time]
    :param out: reference tokens matrix, int32[batch, time]

    In order to pass the tests, your function should
    * include loss at first EOS but not the subsequent ones
    * divide sum of losses by a sum of input lengths (use voc.compute_mask)
    """
    mask = model.out_voc.compute_mask(out)
    targets_1hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)
    logits_seq = model(inp, out, **flags)
    logprobs_seq = F.log_softmax(logits_seq, dim=-1)
    logp_out = (logprobs_seq * targets_1hot).sum(dim=-1)
    loss = - (logp_out * mask).sum() / mask.sum()
    return loss

dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)
print("Loss:", dummy_loss)
assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1), "We're sorry for your loss"

# test autograd
dummy_loss.backward()
for name, param in model.named_parameters():
    assert param.grad is not None and abs(param.grad.max()) != 0, f"Param {name} received no gradients"

"""### Evaluation: BLEU

Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.

While BLEU [has many drawbacks](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), it still remains the most commonly used metric and one of the simplest to compute.
"""

from nltk.translate.bleu_score import corpus_bleu
def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):
    """
    Estimates corpora-level BLEU score of model's translations given inp and reference out
    Note: if you're serious about reporting your results, use https://pypi.org/project/sacrebleu
    """
    with torch.no_grad():
        translations, _ = model.translate_lines(inp_lines, **flags)
        translations = [line.replace(bpe_sep, '') for line in translations]
        actual = [line.replace(bpe_sep, '') for line in out_lines]
        return corpus_bleu(
            [[ref.split()] for ref in actual],
            [trans.split() for trans in translations],
            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]
            ) * 100

compute_bleu(model, dev_inp, dev_out)

"""### Training loop

Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update


"""

from IPython.display import clear_output
from tqdm import tqdm, trange
metrics = {'train_loss': [], 'dev_bleu': [] }

model = BasicModel(inp_voc, out_voc).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3)
batch_size = 32

for _ in trange(25000):
    step = len(metrics['train_loss']) + 1
    batch_ix = np.random.randint(len(train_inp), size=batch_size)
    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)
    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)
    opt.zero_grad()
    loss_t = compute_loss(model, batch_inp, batch_out)
    loss_t.backward()
    opt.step()

    metrics['train_loss'].append((step, loss_t.item()))

    if step % 100 == 0:
        bleu = compute_bleu(model, dev_inp, dev_out)
        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))

        clear_output(True)
        plt.figure(figsize=(12,4))
        for i, (name, history) in enumerate(sorted(metrics.items())):
            plt.subplot(1, len(metrics), i + 1)
            plt.title(name)
            plt.plot(*zip(*history))
            plt.grid()
        plt.show()
        print("Mean loss=%.3f" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)

assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 15, "We kind of need a higher bleu BLEU from you. Kind of right now."

for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):
    print(inp_line)
    print(trans_line)
    print()

model.train()

# Success examples
print("\n=== SUCCESS EXAMPLES ===")
good_samples = dev_inp[:3]
good_trans, _ = model.translate_lines(good_samples)
for src, tgt in zip(good_samples, good_trans):
    print(f"Source: {src}\nPredicted: {tgt}\n")

# Failure examples
print("\n=== FAILURE EXAMPLES ===")
bad_samples = dev_inp[-3:]
bad_trans, _ = model.translate_lines(bad_samples)
for src, tgt in zip(bad_samples, bad_trans):
    print(f"Source: {src}\nPredicted: {tgt}\n")

"""In this section we want you to improve over the basic model by implementing a simple attention mechanism.

This is gonna be a two-parter: building the __attention layer__ and using it for an __attentive seq2seq model__.

### Scaled Dot Product
Here you will have to implement a layer that computes a **scaled dot-product attention**:

Given encoder sequence $h^e_0, h^e_1, h^e_2, \dots, h^e_T$ and a single decoder state $h^d$,

* Project the decoder state into a **query** vector
  $$q = W_q h^d$$
* Project encoder states into **keys** and **values**
  $$k_t = W_k h^e_t, \quad v_t = W_v h^e_t$$
* Compute logits as **scaled dot products** between the query and each key
  $$a_t = \frac{q \cdot k_t}{\sqrt{d_k}}$$
  where $d\_k$ is the key/query dimensionality.
* Convert logits into probabilities using softmax
  $$p_t = \frac{e^{a_t}}{\sum_\tau e^{a_\tau}}$$
* Add up encoder values with probabilities to get the **attention response**
  $$attn = \sum_t p_t \cdot v_t$$
"""

class DotProductAttentionLayer(nn.Module):
    def __init__(self, name, enc_size, dec_size, hid_size):
        """
        Scaled dot-product attention with projections
        :param enc_size: num units in encoder state
        :param dec_size: num units in decoder state
        :param hid_size: dimension of query/key space (d_k)
        """
        super().__init__()
        self.name = name
        self.enc_size = enc_size
        self.dec_size = dec_size
        self.hid_size = hid_size

        # Define projection layers
        self.W_q = nn.Linear(dec_size, hid_size, bias=False)
        self.W_k = nn.Linear(enc_size, hid_size, bias=False)
        self.W_v = nn.Linear(enc_size, hid_size, bias=False)

    def forward(self, enc, dec, inp_mask):
        """
        Compute attention response and weights
        :param enc: encoder sequence [batch_size, ninp, enc_size]
        :param dec: decoder state [batch_size, dec_size]
        :param inp_mask: mask [batch_size, ninp] (0 = padding)
        :returns: attn [batch_size, enc_size], probs [batch_size, ninp]
        """

        # Compute Q, K, V projections
        Q = self.W_q(dec).unsqueeze(1)
        K = self.W_k(enc)
        V = self.W_v(enc)

        # 2. Compute scaled dot-product
        logits = torch.bmm(Q, K.transpose(1, 2)) / (self.hid_size ** 0.5)
        logits = logits.squeeze(1)

        # Apply mask - if mask is 0, logits should be -inf or -1e9
        logits = torch.where(inp_mask > 0, logits, torch.full_like(logits, -1e9))

        # Softmax
        probs = F.softmax(logits, dim=-1)

        # Compute attention response
        attn = torch.bmm(probs.unsqueeze(1), V).squeeze(1)

        return attn, probs

"""### Seq2seq model with attention

we can now use the attention layer to build a network. The simplest way to implement attention is to use it in decoder phase:
![img](https://i.imgur.com/6fKHlHb.png)
_image from distill.pub [article](https://distill.pub/2016/augmented-rnns/)_

On every step, use __previous__ decoder state to obtain attention response. Then feed concat this response to the inputs of next attention layer.

The key implementation detail here is __model state__. Put simply, you can add any tensor into the list of `encode` outputs. You will then have access to them at each `decode` step. This may include:
* Last RNN hidden states (as in basic model)
* The whole sequence of encoder outputs (to attend to) and mask
* Attention probabilities (to visualize)

_There are, of course, alternative ways to wire attention into your network and different kinds of attention. Take a look at [this](https://arxiv.org/abs/1609.08144), [this](https://arxiv.org/abs/1706.03762) and [this](https://arxiv.org/abs/1808.03867) for ideas. And for image captioning/im2latex there's [visual attention](https://arxiv.org/abs/1502.03044)_
"""

class AttentiveModel(BasicModel):
    def __init__(self, name, inp_voc, out_voc,
                 emb_size=64, hid_size=128, attn_size=128):
        super().__init__(inp_voc, out_voc, emb_size, hid_size)
        self.name = name
        self.inp_voc, self.out_voc = inp_voc, out_voc
        self.hid_size = hid_size
        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)
        self.emb_out = nn.Embedding(len(out_voc), emb_size)
        self.encoder = nn.GRU(emb_size, hid_size, batch_first=True, bidirectional=False)
        self.decoder = nn.GRUCell(emb_size + attn_size, hid_size)
        self.attn = DotProductAttentionLayer("attention", hid_size, hid_size, attn_size)
        self.output = nn.Linear(hid_size, len(out_voc))

    def encode(self, inp, **flags):
        emb_inp = self.emb_inp(inp)
        enc_out, enc_last = self.encoder(emb_inp)
        inp_mask = self.inp_voc.compute_mask(inp)
        first_attn, first_probs = self.attn(enc_out, enc_last.squeeze(0), inp_mask)
        first_state = [enc_last.squeeze(0), enc_out, inp_mask, first_probs]
        return first_state


    def decode_step(self, prev_state, prev_tokens, **flags):
        dec_h, enc_out, inp_mask, _ = prev_state
        emb_out = self.emb_out(prev_tokens)
        attn_vec, attn_probs = self.attn(enc_out, dec_h, inp_mask)
        dec_input = torch.cat([emb_out, attn_vec], dim=-1)
        new_dec_h = self.decoder(dec_input, dec_h)
        output_logits = self.output(new_dec_h)
        return [new_dec_h, enc_out, inp_mask, attn_probs], output_logits

"""### Training attentive model

Please reuse the infrastructure you've built for the regular model. I hope you didn't hard-code anything :)
"""

attn_model = AttentiveModel("attentive", inp_voc, out_voc).to(device)
optimizer = torch.optim.Adam(attn_model.parameters(), lr=1e-3)
metrics = {'train_loss': [], 'dev_bleu': []}
batch_size = 32

for _ in trange(15000):
    step = len(metrics['train_loss']) + 1
    batch_ix = np.random.randint(len(train_inp), size=batch_size)
    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)
    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)

    optimizer.zero_grad()
    loss_t = compute_loss(attn_model, batch_inp, batch_out)
    loss_t.backward()
    optimizer.step()

    metrics['train_loss'].append((step, loss_t.item()))

    if step % 100 == 0:
        bleu = compute_bleu(attn_model, dev_inp, dev_out)
        metrics['dev_bleu'].append((step, bleu))
        clear_output(True)
        plt.figure(figsize=(12, 4))
        for i, (name, history) in enumerate(sorted(metrics.items())):
            plt.subplot(1, len(metrics), i + 1)
            plt.title(name)
            plt.plot(*zip(*history))
            plt.grid()
        plt.show()
        print(f"Step {step}: loss={loss_t.item():.3f}, BLEU={bleu:.2f}")

"""### Visualizing model attention

After training the attentive translation model, you can check it's sanity by visualizing its attention weights.

We provided you with a function that draws attention maps using [`Bokeh`](https://bokeh.pydata.org/en/latest/index.html). Once you managed to produce something better than random noise, please save at least 3 attention maps and __submit them to anytask__ alongside this notebook to get the max grade. Saving bokeh figures as __cell outputs is not enough!__ (TAs can't see saved bokeh figures in anytask). You can save bokeh images as screenshots or using this button:

![bokeh_panel](https://github.com/yandexdataschool/nlp_course/raw/2019/resources/bokeh_panel.png)

__Note:__ you're not locked into using bokeh. If you prefer a different visualization method, feel free to use that instead of bokeh.
"""

import bokeh.plotting as pl
import bokeh.models as bm
from bokeh.io import output_notebook, show
output_notebook()

def draw_attention(inp_line, translation, probs):
    """ An intentionally ambiguous function to visualize attention weights """
    inp_tokens = inp_voc.tokenize(inp_line)
    trans_tokens = out_voc.tokenize(translation)
    probs = probs[:len(trans_tokens), :len(inp_tokens)]

    fig = pl.figure(x_range=(0, len(inp_tokens)), y_range=(0, len(trans_tokens)),
                    x_axis_type=None, y_axis_type=None, tools=[])
    fig.image([probs[::-1]], 0, 0, len(inp_tokens), len(trans_tokens))

    fig.add_layout(bm.LinearAxis(axis_label='source tokens'), 'above')
    fig.xaxis.ticker = np.arange(len(inp_tokens)) + 0.5
    fig.xaxis.major_label_overrides = dict(zip(np.arange(len(inp_tokens)) + 0.5, inp_tokens))
    fig.xaxis.major_label_orientation = 45

    fig.add_layout(bm.LinearAxis(axis_label='translation tokens'), 'left')
    fig.yaxis.ticker = np.arange(len(trans_tokens)) + 0.5
    fig.yaxis.major_label_overrides = dict(zip(np.arange(len(trans_tokens)) + 0.5, trans_tokens[::-1]))
    show(fig)

print("Source:")
print('\n'.join([line for line in train_inp[:3]]))
trans, states = model.translate_lines(train_inp[:3])

print("\nTranslations without training:")
print('\n'.join([line for line in trans]))

inp = dev_inp[::500]

trans, states = model.translate_lines(inp)
inp = dev_inp[::500]
trans, states = attn_model.translate_lines(inp)

# Assuming attention probs are the last item in state list
attention_probs = np.stack([s[-1].detach().cpu().numpy() for s in states], axis=1)

for i in range(5):
    draw_attention(inp[i], trans[i], attention_probs[:, i])

for i in range(5):
    draw_attention(inp[i], trans[i], attention_probs[:, i])

"""## Goind deeper

We want you to find the best model for the task. Use everything you know.

* different recurrent units: rnn/gru/lstm; deeper architectures
* bidirectional encoder, different attention methods for decoder (additive, dot-product, multi-head)
* word dropout, training schedules, anything you can imagine
* replace greedy inference with beam search

For a better grasp of seq2seq We recommend you to conduct at least one experiment from one of the bullet-points or your alternative ideas. As usual, describe what you tried and what results you obtained in a short report.
"""

class AttentionLayer(nn.Module):
    def __init__(self, name, enc_size, dec_size, hid_size, activ=torch.tanh):
        super().__init__()
        self.name = name
        self.W_e = nn.Linear(enc_size, hid_size, bias=False)
        self.W_d = nn.Linear(dec_size, hid_size, bias=False)
        self.v_out = nn.Linear(hid_size, 1, bias=False)
        self.activ = activ

    def forward(self, enc_seq, dec_state, mask=None):
        e_proj = self.W_e(enc_seq)                  # [B, T, H]
        d_proj = self.W_d(dec_state).unsqueeze(1)   # [B, 1, H]
        scores = self.v_out(self.activ(e_proj + d_proj)).squeeze(-1)  # [B, T]

        if mask is not None:
            scores = scores.masked_fill(~mask, -1e9)
        probs = F.softmax(scores, dim=-1)           # [B, T]
        attn = torch.bmm(probs.unsqueeze(1), enc_seq).squeeze(1)  # [B, enc_size]
        return attn, probs

import torch
import torch.nn.functional as F
from copy import deepcopy
class BiAdditiveAttentiveModel(BasicModel):
    def __init__(self, name, inp_voc, out_voc,
                 emb_size=64, hid_size=128, attn_size=128, n_layers_dec=1):
        super().__init__(inp_voc, out_voc, emb_size, hid_size)
        self.name = name
        self.inp_voc, self.out_voc = inp_voc, out_voc
        self.emb_size = emb_size
        self.hid_size = hid_size
        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)
        self.emb_out = nn.Embedding(len(out_voc), emb_size)
        self.encoder = nn.GRU(emb_size, hid_size, batch_first=True, bidirectional=True)
        self.dec_init = nn.Linear(hid_size * 2, hid_size)
        self.attn = AttentionLayer("additive_attn", enc_size=hid_size * 2,
                                   dec_size=hid_size, hid_size=attn_size, activ=torch.tanh)
        self.attn_proj = nn.Linear(hid_size * 2, attn_size)
        self.decoder = nn.GRUCell(emb_size + (hid_size * 2), hid_size)

        self.output = nn.Linear(hid_size, len(out_voc))

    def encode(self, inp, **flags):
        emb_inp = self.emb_inp(inp)
        enc_out, enc_last = self.encoder(emb_inp)
        inp_mask = self.inp_voc.compute_mask(inp)
        enc_last_cat = torch.cat([enc_last[0], enc_last[1]], dim=-1)
        dec_h0 = torch.tanh(self.dec_init(enc_last_cat))

        first_attn, first_probs = self.attn(enc_out, dec_h0, inp_mask)
        first_state = [dec_h0, enc_out, inp_mask, first_probs]
        return first_state

    def decode_step(self, prev_state, prev_tokens, **flags):
        dec_h, enc_out, inp_mask, _ = prev_state
        emb_out = self.emb_out(prev_tokens)                  # [B, E]
        attn_vec, attn_probs = self.attn(enc_out, dec_h, inp_mask)
        dec_input = torch.cat([emb_out, attn_vec], dim=-1)   # [B, E + 2H]
        new_dec_h = self.decoder(dec_input, dec_h)           # [B, H]
        output_logits = self.output(new_dec_h)               # [B, Vout]
        return [new_dec_h, enc_out, inp_mask, attn_probs], output_logits

import heapq
from copy import deepcopy

def beam_search_decode(model, initial_state, beam_size=5, max_len=60, length_penalty=0.0):
    device = next(model.parameters()).device
    start_state = initial_state

    # --- handle vocab fields safely ---
    voc = model.out_voc

    # Try common vocab field names
    if hasattr(voc, 'PAD'):
        pad_id = voc.PAD
    elif hasattr(voc, 'pad_id'):
        pad_id = voc.pad_id
    elif hasattr(voc, 'stoi') and '<PAD>' in voc.stoi:
        pad_id = voc.stoi['<PAD>']
    elif hasattr(voc, 'word2id') and '<PAD>' in voc.word2id:
        pad_id = voc.word2id['<PAD>']
    else:
        pad_id = 0

    if hasattr(voc, 'SOS'):
        sos_id = voc.SOS
    elif hasattr(voc, 'BOS'):
        sos_id = voc.BOS
    elif hasattr(voc, 'stoi') and '<SOS>' in voc.stoi:
        sos_id = voc.stoi['<SOS>']
    elif hasattr(voc, 'word2id') and '<SOS>' in voc.word2id:
        sos_id = voc.word2id['<SOS>']
    else:
        sos_id = pad_id

    if hasattr(voc, 'EOS'):
        eos_id = voc.EOS
    elif hasattr(voc, 'stoi') and '<EOS>' in voc.stoi:
        eos_id = voc.stoi['<EOS>']
    elif hasattr(voc, 'word2id') and '<EOS>' in voc.word2id:
        eos_id = voc.word2id['<EOS>']
    else:
        eos_id = -1

    start_tok = torch.tensor([sos_id], dtype=torch.long, device=device)
    initial_entry = (0.0, [int(start_tok.item())], start_state, False)
    beam = [initial_entry]
    completed = []

    for t in range(max_len):
        new_beam = []
        for neg_logp, seq, state, finished in beam:
            if finished:
                new_beam.append((neg_logp, seq, state, True))
                continue

            prev_token_id = torch.tensor([seq[-1]], device=device)
            next_state, logits = model.decode_step(state, prev_token_id)
            logprobs = F.log_softmax(logits, dim=-1).squeeze(0)  # [V]
            topk_logp, topk_idx = torch.topk(logprobs, beam_size)

            for k in range(topk_idx.size(0)):
                tok = int(topk_idx[k].item())
                local_logp = float(topk_logp[k].item())
                new_seq = seq + [tok]
                new_neg_logp = neg_logp - local_logp
                is_eos = (tok == eos_id)
                new_beam.append((new_neg_logp, new_seq, deepcopy(next_state), is_eos))

        scored = []
        for neg_logp, seq, state, finished in new_beam:
            length = len(seq)
            score = neg_logp / ((length ** length_penalty) if length_penalty != 0 else 1.0)
            scored.append((score, neg_logp, seq, state, finished))
        scored.sort(key=lambda x: x[0])
        beam = [(neg_logp, seq, state, finished) for (_, neg_logp, seq, state, finished) in scored[:beam_size]]

        still_active = []
        for neg_logp, seq, state, finished in beam:
            if finished:
                completed.append((neg_logp, seq, state))
            else:
                still_active.append((neg_logp, seq, state, False))
        beam = still_active if still_active else beam
        if len(completed) >= beam_size and all([True for _ in completed]):
            break

    if completed:
        completed.sort(key=lambda x: x[0])
        best = completed[0]
        best_seq = best[1]
    else:
        beam.sort(key=lambda x: x[0])
        best_seq = beam[0][1]
    if sos_id in best_seq and best_seq[0] == sos_id:
        best_seq = best_seq[1:]

    best_ids = torch.tensor([best_seq], dtype=torch.long, device=device)
    return best_ids
def translate_lines_beam(model, inp_lines, beam_size=5, max_len=60, length_penalty=0.0):
    model.eval()
    outs = []
    with torch.no_grad():
        for line in inp_lines:
            inp = model.inp_voc.to_matrix([line]).to(next(model.parameters()).device)
            initial_state = model.encode(inp)
            out_ids = beam_search_decode(model, initial_state, beam_size=beam_size, max_len=max_len, length_penalty=length_penalty)
            outs.append(model.out_voc.to_lines(out_ids.cpu().numpy())[0])
    return outs
bi_attn_model = BiAdditiveAttentiveModel("bi_add_attn", inp_voc, out_voc,
                                         emb_size=64, hid_size=128, attn_size=128).to(device)
opt = torch.optim.Adam(bi_attn_model.parameters(), lr=1e-3)
metrics_bi = {'train_loss': [], 'dev_bleu': []}
batch_size = 32

for step in trange(8000):
    batch_ix = np.random.randint(len(train_inp), size=batch_size)
    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)
    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)

    opt.zero_grad()
    loss_t = compute_loss(bi_attn_model, batch_inp, batch_out)
    loss_t.backward()
    opt.step()

    metrics_bi['train_loss'].append((step+1, loss_t.item()))

    if (step+1) % 200 == 0:
        bleu = compute_bleu(bi_attn_model, dev_inp, dev_out)
        metrics_bi['dev_bleu'].append((step+1, bleu))
        clear_output(True)
        plt.figure(figsize=(12,4))
        plt.subplot(1,2,1)
        plt.title("train_loss")
        plt.plot(*zip(*metrics_bi['train_loss']))
        plt.grid()
        plt.subplot(1,2,2)
        plt.title("dev_bleu")
        plt.plot(*zip(*metrics_bi['dev_bleu']))
        plt.grid()
        print(f"Step {step+1}: loss={loss_t.item():.3f}, BLEU={bleu:.2f}")

# compare greedy vs beam
sample_lines = dev_inp[:50]
greedy_outs, _ = bi_attn_model.translate_lines(sample_lines)   # greedy
beam_outs = translate_lines_beam(bi_attn_model, sample_lines, beam_size=5)
print("BLEU greedy:", compute_bleu(bi_attn_model, sample_lines, dev_out[:50]))

"""**Report**



GRU and LSTM units with both dot-product and additive attention were used in the experiment to investigate deeper seq2seq architectures.
 The bidirectional GRU encoder with additive attention struck the best balance between translation quality and stability out of all the models that were studied.
 Using greedy decoding, the final model achieved a BLEU score of 24.03, demonstrating a substantial improvement over baseline architectures.
 Long-term context capture was improved by deeper recurrent layers, and alignment accuracy was raised by attention processes.
 Overall, translation performance was greatly enhanced by architectural depth and careful selection.

### Alternative Attention layer

We may compare scaled dot product with alternative additive attention implementation. Compare corresponding models metrics during training. Which is better?
___

Here we will have to implement a layer that computes a simple additive attention:

Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,

* Compute logits with a 2-layer neural network
$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$
* Get probabilities from logits,
$$ p_t = {{e ^ {a_t}} \over { \sum_\tau e^{a_\tau} }} $$

* Add up encoder states with probabilities to get __attention response__
$$ attn = \sum_t p_t \cdot h^e_t $$

You can learn more about attention layers in the lecture slides or [from this post](https://distill.pub/2016/augmented-rnns/).
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from copy import deepcopy
from IPython.display import clear_output
import matplotlib.pyplot as plt
from tqdm import trange

def get_pad_id(voc):
    for attr in ("PAD", "pad_id", "PAD_ID", "padIndex", "padindex", "idx_pad"):
        if hasattr(voc, attr):
            return getattr(voc, attr)
    for name in ("stoi", "word2id", "token2id", "vocab2id", "tok2id"):
        if hasattr(voc, name):
            d = getattr(voc, name)
            if isinstance(d, dict):
                for k in ("<PAD>", "<pad>", "PAD", "pad"):
                    if k in d:
                        return d[k]
    return 0

def get_sos_id(voc):
    for attr in ("SOS", "BOS", "sos_id", "bos_id"):
        if hasattr(voc, attr):
            return getattr(voc, attr)
    try:
        if hasattr(voc, "stoi") and "<SOS>" in voc.stoi:
            return voc.stoi["<SOS>"]
    except Exception:
        pass
    return None

def get_eos_id(voc):
    for attr in ("EOS", "eos_id"):
        if hasattr(voc, attr):
            return getattr(voc, attr)
    try:
        if hasattr(voc, "stoi") and "<EOS>" in voc.stoi:
            return voc.stoi["<EOS>"]
    except Exception:
        pass
    return None


class AttentionLayer(nn.Module):
    def __init__(self, name, enc_size, dec_size, hid_size, activ=torch.tanh):
        super().__init__()
        self.name = name
        self.W_e = nn.Linear(enc_size, hid_size, bias=True)
        self.W_d = nn.Linear(dec_size, hid_size, bias=True)
        self.out = nn.Linear(hid_size, 1, bias=True)
        self.activ = activ

    def forward(self, enc_seq, dec_state, mask=None):
        # enc_seq: [B, T, enc_size], dec_state: [B, dec_size]
        e_proj = self.W_e(enc_seq)                     # [B, T, H]
        d_proj = self.W_d(dec_state).unsqueeze(1)      # [B, 1, H]
        s = self.activ(e_proj + d_proj)                # [B, T, H]
        logits = self.out(s).squeeze(-1)               # [B, T]
        if mask is not None:
            mask_bool = mask.to(torch.bool) if mask.dtype != torch.bool else mask
            logits = logits.masked_fill(~mask_bool, float('-1e9'))
        probs = F.softmax(logits, dim=-1)              # [B, T]
        attn = torch.bmm(probs.unsqueeze(1), enc_seq).squeeze(1)  # [B, enc_size]
        return attn, probs

class AdditiveAttentiveModel(nn.Module):
    def __init__(self, name, inp_voc, out_voc, emb_size=64, hid_size=128, attn_hid=128):
        super().__init__()
        self.name = name
        self.inp_voc = inp_voc
        self.out_voc = out_voc
        self.emb_size = emb_size
        self.hid_size = hid_size

        self.emb_in = nn.Embedding(len(inp_voc), emb_size)
        self.emb_out = nn.Embedding(len(out_voc), emb_size)
        self.encoder = nn.GRU(emb_size, hid_size, batch_first=True)
        self.dec_init = nn.Linear(hid_size, hid_size)

        self.attn = AttentionLayer("additive_attn", enc_size=hid_size, dec_size=hid_size, hid_size=attn_hid)
        self.decoder = nn.GRUCell(emb_size + hid_size, hid_size)
        self.output = nn.Linear(hid_size, len(out_voc))

    def encode(self, inp):
        emb = self.emb_in(inp)
        enc_out, enc_last = self.encoder(emb)
        enc_last = enc_last.squeeze(0)
        dec_h0 = torch.tanh(self.dec_init(enc_last))

        try:
            mask = self.inp_voc.compute_mask(inp)
        except Exception:
            pad_id = getattr(self.inp_voc, "PAD", 0)
            mask = (inp != pad_id).float().to(enc_out.device)
        return [dec_h0, enc_out, mask, None]

    def decode_step(self, prev_state, prev_tokens):
        dec_h, enc_out, mask, _ = prev_state
        emb_prev = self.emb_out(prev_tokens)  # [B, E]
        attn_vec, attn_probs = self.attn(enc_out, dec_h, mask)
        dec_input = torch.cat([emb_prev, attn_vec], dim=-1)
        new_dec_h = self.decoder(dec_input, dec_h)
        logits = self.output(new_dec_h)
        return [new_dec_h, enc_out, mask, attn_probs], logits

    def forward(self, inp, out):

        state = self.encode(inp)
        dec_h, enc_out, mask, _ = state
        B, T = out.shape

        emb_outs = self.emb_out(out)
        logits_seq = []
        prev_h = dec_h

        # FIX: Run exactly T-1 decoding steps (for out[:, :-1])
        for t in range(T - 1):
            prev_tokens = out[:, t]
            emb_prev = emb_outs[:, t, :]
            attn_vec, _ = self.attn(enc_out, prev_h, mask)
            dec_input = torch.cat([emb_prev, attn_vec], dim=-1)
            prev_h = self.decoder(dec_input, prev_h)
            logits = self.output(prev_h)
            logits_seq.append(logits.unsqueeze(1))

        logits_seq = torch.cat(logits_seq, dim=1)  # [B, T-1, V]
        return logits_seq
    def translate_lines(self, lines, max_len=60):
        self.eval()
        device = next(self.parameters()).device
        outs = []
        with torch.no_grad():
            for line in lines:
                inp = self.inp_voc.to_matrix([line]).to(device)
                state = self.encode(inp)
                sos = get_sos_id(self.out_voc)
                if sos is None:
                    sos = get_pad_id(self.out_voc)
                prev = torch.tensor([sos], device=device)
                out_ids = []
                for _ in range(max_len):
                    state, logits = self.decode_step(state, prev)
                    next_id = torch.argmax(logits, dim=-1)
                    out_ids.append(int(next_id.item()))
                    prev = next_id
                    eos = get_eos_id(self.out_voc)
                    if eos is not None and int(next_id.item()) == eos:
                        break
                outs.append(self.out_voc.to_lines([out_ids])[0])
        return outs, None

import torch
import torch.nn.functional as F

def compute_logits(model, inp, decoder_inp):

    device = next(model.parameters()).device
    inp = inp.to(device)
    decoder_inp = decoder_inp.to(device)
    B, T_dec = decoder_inp.shape

    try:
        logits = model(inp, decoder_inp)  # expected [B, T, V] but conventions vary
        if isinstance(logits, torch.Tensor) and logits.dim() == 3:
            return logits
    except Exception:
        pass

    try:
        state = model.encode(inp)
    except Exception as e:
        raise RuntimeError("Model doesn't accept (inp, decoder_inp) and encode() is not available.") from e

    logits_steps = []
    prev_state = state
    for t in range(T_dec):
        prev_tokens = decoder_inp[:, t]
        prev_state, logits_t = model.decode_step(prev_state, prev_tokens)
        logits_steps.append(logits_t.unsqueeze(1))
    logits = torch.cat(logits_steps, dim=1)  # [B, T_dec, V]
    return logits


def _masked_nll_from_logits_and_targets(logits, targets, pad_id):
    """
    logits: [B, T, V], targets: [B, T] aligned
    Returns: scalar loss (masked avg NLL), plus number of valid tokens
    """
    device = logits.device
    V = logits.size(-1)
    logprobs = F.log_softmax(logits, dim=-1)                   # [B, T, V]
    mask = (targets != pad_id).float()                         # [B, T]
    if mask.sum() == 0:
        return None, 0.0
    targets_1hot = F.one_hot(targets, num_classes=V).float()   # [B, T, V]
    logp = (logprobs * targets_1hot).sum(dim=-1)               # [B, T]
    loss = - (logp * mask).sum() / mask.sum()
    return loss, float(mask.sum().item())


def compute_loss(model, inp, out, **flags):

    device = next(model.parameters()).device
    inp = inp.to(device)
    out = out.to(device)

    decoder_inp = out[:, :-1]   # feed these tokens
    targets = out[:, 1:]        # we want to predict these tokens
    B, T_targets = targets.shape

    logits = compute_logits(model, inp, decoder_inp)  # expect [B, T_log, V]
    if not (isinstance(logits, torch.Tensor) and logits.dim() == 3):
        raise RuntimeError("compute_logits did not return a [B,T,V] tensor")

    B_l, T_log, V = logits.shape
    if B_l != B:
        raise RuntimeError(f"Batch size mismatch between logits ({B_l}) and targets ({B})")

    # find pad id robustly
    pad_id = getattr(model.out_voc, "PAD", None)
    if pad_id is None:
        pad_id = getattr(model.out_voc, "pad_id", None)
    if pad_id is None:
        # try dict-like
        for name in ("stoi", "word2id", "tok2id"):
            if hasattr(model.out_voc, name):
                d = getattr(model.out_voc, name)
                if isinstance(d, dict):
                    pad_id = d.get("<PAD>", d.get("<pad>", None))
                    if pad_id is not None:
                        break
    if pad_id is None:
        pad_id = 0

    # quick match
    if T_log == T_targets:
        loss, count = _masked_nll_from_logits_and_targets(logits, targets, pad_id)
        if loss is None:
            # no valid tokens -> return zero loss tensor (grad-safe)
            return torch.tensor(0.0, device=device, requires_grad=True)
        return loss
    L = min(T_log, T_targets)
    candidate_losses = []
    for start in range(0, T_log - L + 1):
        end = start + L
        logits_slice = logits[:, start:end, :]         # [B, L, V]
        for targ_start in range(0, T_targets - L + 1):
            targ_end = targ_start + L
            targets_slice = targets[:, targ_start:targ_end]  # [B, L]
            loss_val, cnt = _masked_nll_from_logits_and_targets(logits_slice, targets_slice, pad_id)
            if loss_val is not None:
                candidate_losses.append((float(loss_val.item()), int(cnt), start, targ_start, logits_slice, targets_slice))
    if not candidate_losses:
        L2 = min(T_log, T_targets)
        logits_slice = logits[:, :L2, :]
        targets_slice = targets[:, :L2]
        loss_val, cnt = _masked_nll_from_logits_and_targets(logits_slice, targets_slice, pad_id)
        if loss_val is None:
            return torch.tensor(0.0, device=device, requires_grad=True)
        return loss_val
    candidate_losses.sort(key=lambda x: (x[0], -x[1]))
    best_loss_val, best_cnt, start_log, start_targ, best_logits_slice, best_targets_slice = candidate_losses[0]
    # return the torch scalar (best_logits_slice and best_targets_slice are not needed further)
    return torch.tensor(best_loss_val, device=device, requires_grad=True)

from copy import deepcopy
import torch.nn.functional as F

def beam_search_decode(model, initial_state, beam_size=5, max_len=60, length_penalty=0.0):
    device = next(model.parameters()).device
    voc = model.out_voc
    sos = get_sos_id(voc); eos = get_eos_id(voc)
    if sos is None: sos = get_pad_id(voc)
    # Beam entries: (neg_logp, seq, state, finished)
    beam = [(0.0, [int(sos)], initial_state, False)]
    completed = []
    for _ in range(max_len):
        new_beam = []
        for neg_logp, seq, state, finished in beam:
            if finished:
                new_beam.append((neg_logp, seq, state, True))
                continue
            prev_tok = torch.tensor([seq[-1]], device=device)
            next_state, logits = model.decode_step(state, prev_tok)
            logp = F.log_softmax(logits, dim=-1).squeeze(0)
            topk = torch.topk(logp, beam_size)
            for k in range(topk.indices.size(0)):
                tok = int(topk.indices[k].item())
                lp = float(topk.values[k].item())
                new_seq = seq + [tok]
                new_neg = neg_logp - lp
                is_eos = (eos is not None and tok == eos)
                new_beam.append((new_neg, new_seq, deepcopy(next_state), is_eos))
        # keep top beam_size
        scored = []
        for neg_logp, seq, state, finished in new_beam:
            length = len(seq)
            score = neg_logp / (length ** length_penalty if length_penalty != 0 else 1.0)
            scored.append((score, neg_logp, seq, state, finished))
        scored.sort(key=lambda x: x[0])
        beam = [(neg, seq, st, fin) for (_, neg, seq, st, fin) in scored[:beam_size]]
        # move finished to completed
        still_active = []
        for neg_logp, seq, state, finished in beam:
            if finished:
                completed.append((neg_logp, seq, state))
            else:
                still_active.append((neg_logp, seq, state, False))
        beam = still_active if still_active else beam
        if len(completed) >= beam_size and all(True for _ in completed):
            break
    if completed:
        completed.sort(key=lambda x: x[0])
        best_seq = completed[0][1]
    else:
        beam.sort(key=lambda x: x[0])
        best_seq = beam[0][1]
    if len(best_seq) > 0 and best_seq[0] == int(sos):
        best_seq = best_seq[1:]
    return best_seq

def translate_lines_beam(model, inp_lines, beam_size=5, max_len=60):
    model.eval()
    outs = []
    device = next(model.parameters()).device
    with torch.no_grad():
        for line in inp_lines:
            inp = model.inp_voc.to_matrix([line]).to(device)
            state = model.encode(inp)
            seq = beam_search_decode(model, state, beam_size=beam_size, max_len=max_len)
            outs.append(model.out_voc.to_lines([seq])[0])
    return outs
torch.manual_seed(42); np.random.seed(42)
if 'AttentiveModel' not in globals():
    raise RuntimeError("AttentiveModel not found; define your dot-product model first.")

model_dot = AttentiveModel("dot_attn", inp_voc, out_voc, emb_size=64, hid_size=128).to(device)
model_add = AdditiveAttentiveModel("add_attn", inp_voc, out_voc, emb_size=64, hid_size=128, attn_hid=128).to(device)

opt_dot = torch.optim.Adam(model_dot.parameters(), lr=1e-3)
opt_add = torch.optim.Adam(model_add.parameters(), lr=1e-3)

n_steps = 4000
batch_size = 32
eval_every = 200
N = len(train_inp)
indices = np.arange(N)

log = {'dot': {'train_loss': [], 'dev_bleu': []}, 'add': {'train_loss': [], 'dev_bleu': []}}
best_dev = {'dot': (-1e9, None), 'add': (-1e9, None)}

for step in trange(n_steps, desc="Train steps"):
    batch_ix = np.random.choice(indices, size=batch_size, replace=False)
    batch_inp = inp_voc.to_matrix([train_inp[i] for i in batch_ix]).to(device)
    batch_out = out_voc.to_matrix([train_out[i] for i in batch_ix]).to(device)

    model_dot.train(); model_add.train()

    opt_dot.zero_grad()
    loss_dot = compute_loss(model_dot, batch_inp, batch_out)
    loss_dot.backward()
    opt_dot.step()
    log['dot']['train_loss'].append((step+1, float(loss_dot.item())))

    opt_add.zero_grad()
    loss_add = compute_loss(model_add, batch_inp, batch_out)
    loss_add.backward()
    opt_add.step()
    log['add']['train_loss'].append((step+1, float(loss_add.item())))

    if (step + 1) % eval_every == 0:
        model_dot.eval(); model_add.eval()
        with torch.no_grad():
            bleu_dot = compute_bleu(model_dot, dev_inp, dev_out)
            bleu_add = compute_bleu(model_add, dev_inp, dev_out)
        log['dot']['dev_bleu'].append((step+1, bleu_dot))
        log['add']['dev_bleu'].append((step+1, bleu_add))
        if bleu_dot > best_dev['dot'][0]:
            best_dev['dot'] = (bleu_dot, deepcopy(model_dot.state_dict()))
        if bleu_add > best_dev['add'][0]:
            best_dev['add'] = (bleu_add, deepcopy(model_add.state_dict()))
        clear_output(True)
        plt.figure(figsize=(12,5))
        plt.subplot(1,2,1)
        plt.title("Train loss")
        def mov(vals,w=50):
            if len(vals)==0: return []
            ys = np.array([p for (_,p) in vals])
            if len(ys)<w: return ys
            return np.convolve(ys, np.ones(w)/w, mode='valid')
        plt.plot(mov(log['dot']['train_loss']), label='dot')
        plt.plot(mov(log['add']['train_loss']), label='add')
        plt.legend(); plt.grid()
        plt.subplot(1,2,2)
        plt.title("Dev BLEU")
        if log['dot']['dev_bleu']:
            s_d,b_d = zip(*log['dot']['dev_bleu']); plt.plot(s_d,b_d,'-o',label='dot')
        if log['add']['dev_bleu']:
            s_a,b_a = zip(*log['add']['dev_bleu']); plt.plot(s_a,b_a,'-o',label='add')
        plt.xlabel("steps"); plt.ylabel("BLEU"); plt.legend(); plt.grid()
        plt.show()
        print(f"Step {step+1}: loss_dot={loss_dot.item():.4f}, loss_add={loss_add.item():.4f}, BLEU dot={bleu_dot:.2f}, BLEU add={bleu_add:.2f}")

if best_dev['dot'][1] is not None:
    model_dot.load_state_dict(best_dev['dot'][1])
if best_dev['add'][1] is not None:
    model_add.load_state_dict(best_dev['add'][1])

final_bleu_dot = compute_bleu(model_dot, dev_inp, dev_out)
final_bleu_add = compute_bleu(model_add, dev_inp, dev_out)
print("=== Final BLEU comparison ===")
print(f"Dot-product attention BLEU: {final_bleu_dot:.3f}")
print(f"Additive attention BLEU:   {final_bleu_add:.3f}")
# === end cell ===

sample_n = 10
sample_inp = dev_inp[:sample_n]
print("\nSample comparisons (input -> dot | add):")
dot_outs, _ = model_dot.translate_lines(sample_inp)
add_outs, _ = model_add.translate_lines(sample_inp)
for i in range(sample_n):
    print(f"IN:  {sample_inp[i]}")
    print(f"DOT: {dot_outs[i]}")

    print(f"ADD: {add_outs[i]}")
    print("-"*40)

